# -*- coding: utf-8 -*-
"""augmented-woa-pso-neutrosophic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K17Hns_xloOpNY56Ema4CjlbGUdhCoeR
"""

!pip install pyswarm

import numpy as np
import pandas as pd
import re
import random
import nltk
from nltk.corpus import wordnet, stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import log_loss
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
from pyswarm import pso

# Ensure NLTK WordNet and stopwords are downloaded
nltk.download('wordnet')
nltk.download('stopwords')

# Define stop words
stop_words = set(stopwords.words('english'))

# Define the clean_tweet function
def clean_tweet(tweet):
    tweet = re.sub(r'http\S+', '', tweet)  # Remove URLs
    tweet = re.sub(r'@\w+', '', tweet)     # Remove mentions
    tweet = re.sub(r'#\w+', '', tweet)     # Remove hashtags
    tweet = re.sub(r'\d+', '', tweet)      # Remove numbers
    tweet = re.sub(r'[^\w\s]', '', tweet)  # Remove punctuation
    tweet = tweet.lower()                  # Convert to lowercase
    tweet = tweet.strip()                  # Remove leading/trailing whitespaces
    return tweet

# Data augmentation functions
def get_synonyms(word):
    synonyms = set()
    for syn in wordnet.synsets(word):
        for l in syn.lemmas():
            synonym = l.name().replace("_", " ").replace("-", " ").lower()
            synonym = "".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])
            synonyms.add(synonym)
    if word in synonyms:
        synonyms.remove(word)
    return list(synonyms)

def synonym_replacement(words, n):
    new_words = words.copy()
    random_word_list = list(set([word for word in words if word not in stop_words]))
    random.shuffle(random_word_list)
    num_replaced = 0
    for random_word in random_word_list:
        synonyms = get_synonyms(random_word)
        if len(synonyms) >= 1:
            synonym = random.choice(list(synonyms))
            new_words = [synonym if word == random_word else word for word in new_words]
            num_replaced += 1
        if num_replaced >= n:
            break

    sentence = ' '.join(new_words)
    new_words = sentence.split(' ')

    return new_words

def random_deletion(words, p):
    if len(words) == 1:
        return words

    new_words = []
    for word in words:
        r = random.uniform(0, 1)
        if r > p:
            new_words.append(word)

    if len(new_words) == 0:
        rand_int = random.randint(0, len(words)-1)
        return [words[rand_int]]

    return new_words

def random_swap(words, n):
    if len(words) < 2:
        return words
    new_words = words.copy()
    for _ in range(n):
        new_words = swap_word(new_words)
    return new_words

def swap_word(new_words):
    random_idx_1 = random.randint(0, len(new_words)-1)
    random_idx_2 = random_idx_1
    counter = 0
    while random_idx_1 == random_idx_2:
        random_idx_2 = random.randint(0, len(new_words)-1)
        counter += 1
        if counter > 3:
            return new_words
    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]
    return new_words

def random_insertion(words, n):
    new_words = words.copy()
    for _ in range(n):
        add_word(new_words)
    return new_words

def add_word(new_words):
    synonyms = []
    counter = 0
    while len(synonyms) < 1:
        random_word = new_words[random.randint(0, len(new_words)-1)]
        synonyms = get_synonyms(random_word)
        counter += 1
        if counter >= 10:
            return
    random_synonym = random.choice(list(synonyms))
    random_idx = random.randint(0, len(new_words)-1)
    new_words.insert(random_idx, random_synonym)

def augment_sentence(sentence):
    words = sentence.split()
    num_words = len(words)
    if num_words == 0:
        return [sentence]

    augmented_sentences = []
    augmented_sentences.append(' '.join(synonym_replacement(words, max(1, int(0.1*num_words)))))
    augmented_sentences.append(' '.join(random_deletion(words, 0.1)))
    augmented_sentences.append(' '.join(random_swap(words, max(1, int(0.1*num_words)))))
    augmented_sentences.append(' '.join(random_insertion(words, max(1, int(0.1*num_words)))))

    return augmented_sentences

# Load the dataset
file_path = '/content/labeled_data.csv'
data = pd.read_csv(file_path)

# Remove unnecessary columns and clean the data
data = data[['tweet', 'class']]
data['tweet'] = data['tweet'].apply(clean_tweet)

# Encode the labels
label_encoder = LabelEncoder()
data['class'] = label_encoder.fit_transform(data['class'])

# Data augmentation
augmented_data = []

for index, row in data.iterrows():
    tweet = row['tweet']
    label = row['class']
    augmented_sentences = augment_sentence(tweet)
    for augmented_sentence in augmented_sentences:
        augmented_data.append((augmented_sentence, label))

augmented_df = pd.DataFrame(augmented_data, columns=['tweet', 'class'])
data = pd.concat([data, augmented_df])

# Split the data
X_train, X_test, y_train, y_test = train_test_split(data['tweet'], data['class'], test_size=0.2, random_state=42)

# Vectorize the tweet text using TF-IDF
vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Optimize MLP using WOA
def optimize_mlp_woa(X_train, y_train, random_state=42):
    mlp = MLPClassifier(max_iter=50, random_state=random_state)

    def objective(params):
        mlp.set_params(hidden_layer_sizes=(int(params[0]),))
        mlp.fit(X_train, y_train)
        return log_loss(y_train, mlp.predict_proba(X_train))

    bounds = [(50, 500)]  # Hidden layer sizes

    best_params, _ = pso(objective, *zip(*bounds))

    mlp.set_params(hidden_layer_sizes=(int(best_params[0]),))

    mlp.fit(X_train, y_train)

    return mlp

# Optimize MLP using WOA
mlp_optimized_woa_true = optimize_mlp_woa(X_train_tfidf, y_train)
mlp_optimized_woa_false = optimize_mlp_woa(X_train_tfidf, 1 - y_train)

# Particle Swarm Optimization (PSO) for fine-tuning MLP
def fine_tune_mlp_pso(mlp, X_train, y_train):
    def objective(params):
        mlp.set_params(hidden_layer_sizes=(int(params[0]),))
        mlp.fit(X_train, y_train)
        return log_loss(y_train, mlp.predict_proba(X_train))

    bounds = [(50, 500)]  # Hidden layer sizes

    best_params, _ = pso(objective, *zip(*bounds))

    mlp.set_params(hidden_layer_sizes=(int(best_params[0]),))

    mlp.fit(X_train, y_train)

    return mlp

# Fine-tune MLPs using PSO
mlp_true_fine_tuned = fine_tune_mlp_pso(mlp_optimized_woa_true, X_train_tfidf, y_train)
mlp_false_fine_tuned = fine_tune_mlp_pso(mlp_optimized_woa_false, X_train_tfidf, 1 - y_train)

# Custom training loop to capture neutrosophic scores
def train_mlp_neutrosophic(X_train, y_train, mlp_true, mlp_false, max_iter=50):
    train_neutrosophic_scores = []

    for i in range(max_iter):
        T_train = mlp_true.predict_proba(X_train)
        F_train = mlp_false.predict_proba(X_train)

        # Calculate Indeterminacy (I)
        I_train = 1 - T_train - F_train

        train_neutrosophic_scores.append((T_train, I_train, F_train))

    return train_neutrosophic_scores

# Train the models with custom neutrosophic training loop
train_neutrosophic_scores = train_mlp_neutrosophic(X_train_tfidf, y_train, mlp_true_fine_tuned, mlp_false_fine_tuned)

# Extracting loss from neutrosophic scores
train_loss_neutro_true = [log_loss(y_train, T) for T, _, _ in train_neutrosophic_scores]
train_loss_neutro_false = [log_loss(1 - y_train, F) for _, _, F in train_neutrosophic_scores]

# Function to plot training neutrosophic loss curves
def plot_neutrosophic_curves(train_loss_true, train_loss_false):
    plt.figure(figsize=(12, 6))

    # Plot training loss for True Membership Function
    plt.plot(train_loss_true, label='Neutrosophic True Membership Function Training Loss')

    # Plot training loss for False Membership Function
    plt.plot(train_loss_false, label='Neutrosophic False Membership Function Training Loss')

    # Adding labels and legend
    plt.title('Neutrosophic Training Loss Curves')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Plot the training neutrosophic loss curves
plot_neutrosophic_curves(train_loss_neutro_true, train_loss_neutro_false)

!pip install pyswarm

import numpy as np
import pandas as pd
import re
import random
import nltk
from nltk.corpus import wordnet, stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import log_loss
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
from pyswarm import pso

# Ensure NLTK WordNet and stopwords are downloaded
nltk.download('wordnet')
nltk.download('stopwords')

# Define stop words
stop_words = set(stopwords.words('english'))

# Define the clean_tweet function
def clean_tweet(tweet):
    tweet = re.sub(r'http\S+', '', tweet)  # Remove URLs
    tweet = re.sub(r'@\w+', '', tweet)     # Remove mentions
    tweet = re.sub(r'#\w+', '', tweet)     # Remove hashtags
    tweet = re.sub(r'\d+', '', tweet)      # Remove numbers
    tweet = re.sub(r'[^\w\s]', '', tweet)  # Remove punctuation
    tweet = tweet.lower()                  # Convert to lowercase
    tweet = tweet.strip()                  # Remove leading/trailing whitespaces
    return tweet

# Data augmentation functions
def get_synonyms(word):
    synonyms = set()
    for syn in wordnet.synsets(word):
        for l in syn.lemmas():
            synonym = l.name().replace("_", " ").replace("-", " ").lower()
            synonym = "".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])
            synonyms.add(synonym)
    if word in synonyms:
        synonyms.remove(word)
    return list(synonyms)

def synonym_replacement(words, n):
    new_words = words.copy()
    random_word_list = list(set([word for word in words if word not in stop_words]))
    random.shuffle(random_word_list)
    num_replaced = 0
    for random_word in random_word_list:
        synonyms = get_synonyms(random_word)
        if len(synonyms) >= 1:
            synonym = random.choice(list(synonyms))
            new_words = [synonym if word == random_word else word for word in new_words]
            num_replaced += 1
        if num_replaced >= n:
            break

    sentence = ' '.join(new_words)
    new_words = sentence.split(' ')

    return new_words

def random_deletion(words, p):
    if len(words) == 1:
        return words

    new_words = []
    for word in words:
        r = random.uniform(0, 1)
        if r > p:
            new_words.append(word)

    if len(new_words) == 0:
        rand_int = random.randint(0, len(words)-1)
        return [words[rand_int]]

    return new_words

def random_swap(words, n):
    if len(words) < 2:
        return words
    new_words = words.copy()
    for _ in range(n):
        new_words = swap_word(new_words)
    return new_words

def swap_word(new_words):
    random_idx_1 = random.randint(0, len(new_words)-1)
    random_idx_2 = random_idx_1
    counter = 0
    while random_idx_1 == random_idx_2:
        random_idx_2 = random.randint(0, len(new_words)-1)
        counter += 1
        if counter > 3:
            return new_words
    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]
    return new_words

def random_insertion(words, n):
    new_words = words.copy()
    for _ in range(n):
        add_word(new_words)
    return new_words

def add_word(new_words):
    synonyms = []
    counter = 0
    while len(synonyms) < 1:
        random_word = new_words[random.randint(0, len(new_words)-1)]
        synonyms = get_synonyms(random_word)
        counter += 1
        if counter >= 10:
            return
    random_synonym = random.choice(list(synonyms))
    random_idx = random.randint(0, len(new_words)-1)
    new_words.insert(random_idx, random_synonym)

def augment_sentence(sentence):
    words = sentence.split()
    num_words = len(words)
    if num_words == 0:
        return [sentence]

    augmented_sentences = []
    augmented_sentences.append(' '.join(synonym_replacement(words, max(1, int(0.1*num_words)))))
    augmented_sentences.append(' '.join(random_deletion(words, 0.1)))
    augmented_sentences.append(' '.join(random_swap(words, max(1, int(0.1*num_words)))))
    augmented_sentences.append(' '.join(random_insertion(words, max(1, int(0.1*num_words)))))

    return augmented_sentences

# Load the dataset
file_path = '/content/labeled_data.csv'
data = pd.read_csv(file_path)

# Remove unnecessary columns and clean the data
data = data[['tweet', 'class']]
data['tweet'] = data['tweet'].apply(clean_tweet)

# Encode the labels
label_encoder = LabelEncoder()
data['class'] = label_encoder.fit_transform(data['class'])

# Data augmentation
augmented_data = []

for index, row in data.iterrows():
    tweet = row['tweet']
    label = row['class']
    augmented_sentences = augment_sentence(tweet)
    for augmented_sentence in augmented_sentences:
        augmented_data.append((augmented_sentence, label))

augmented_df = pd.DataFrame(augmented_data, columns=['tweet', 'class'])
data = pd.concat([data, augmented_df])

# Split the data
X_train, X_test, y_train, y_test = train_test_split(data['tweet'], data['class'], test_size=0.2, random_state=42)

# Vectorize the tweet text using TF-IDF
vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Whale Optimization Algorithm (WOA) implementation
def woa(objective, bounds, num_whales=30, max_iter=50):
    dim = len(bounds)
    lb = np.array([b[0] for b in bounds])
    ub = np.array([b[1] for b in bounds])

    # Initialize the whales' positions
    whales = lb + (ub - lb) * np.random.rand(num_whales, dim)
    fitness = np.array([objective(w) for w in whales])
    best_idx = np.argmin(fitness)
    best_whale = whales[best_idx]
    best_fitness = fitness[best_idx]

    for t in range(max_iter):
        a = 2 - 2 * t / max_iter
        a2 = -1 + t * (-1) / max_iter

        for i in range(num_whales):
            r1, r2 = np.random.rand(), np.random.rand()
            A = 2 * a * r1 - a
            C = 2 * r2
            b = 1
            l = (a2 - 1) * np.random.rand() + 1

            p = np.random.rand()

            if p < 0.5:
                if abs(A) < 1:
                    D = abs(C * best_whale - whales[i])
                    whales[i] = best_whale - A * D
                else:
                    random_whale = whales[np.random.randint(0, num_whales)]
                    D = abs(C * random_whale - whales[i])
                    whales[i] = random_whale - A * D
            else:
                distance_to_best = abs(best_whale - whales[i])
                whales[i] = distance_to_best * np.exp(b * l) * np.cos(l * 2 * np.pi) + best_whale

            whales[i] = np.clip(whales[i], lb, ub)
            fitness[i] = objective(whales[i])

        best_idx = np.argmin(fitness)
        if fitness[best_idx] < best_fitness:
            best_fitness = fitness[best_idx]
            best_whale = whales[best_idx]

    return best_whale, best_fitness

# Optimize MLP using WOA
def optimize_mlp_woa(X_train, y_train, random_state=42):
    mlp = MLPClassifier(max_iter=50, random_state=random_state)

    def objective(params):
        mlp.set_params(hidden_layer_sizes=(int(params[0]),))
        mlp.fit(X_train, y_train)
        return log_loss(y_train, mlp.predict_proba(X_train))

    bounds = [(50, 500)]  # Hidden layer sizes

    best_params, _ = woa(objective, bounds)

    mlp.set_params(hidden_layer_sizes=(int(best_params[0]),))
    mlp.fit(X_train, y_train)

    return mlp

# Optimize MLP using WOA
mlp_optimized_woa_true = optimize_mlp_woa(X_train_tfidf, y_train)
mlp_optimized_woa_false = optimize_mlp_woa(X_train_tfidf, 1 - y_train)

# Particle Swarm Optimization (PSO) for fine-tuning MLP
def fine_tune_mlp_pso(mlp, X_train, y_train):
    def objective(params):
        mlp.set_params(hidden_layer_sizes=(int(params[0]),))
        mlp.fit(X_train, y_train)
        return log_loss(y_train, mlp.predict_proba(X_train))

    bounds = [(50, 500)]  # Hidden layer sizes

    best_params, _ = pso(objective, *zip(*bounds))

    mlp.set_params(hidden_layer_sizes=(int(best_params[0]),))
    mlp.fit(X_train, y_train)

    return mlp

# Fine-tune MLPs using PSO
mlp_true_fine_tuned = fine_tune_mlp_pso(mlp_optimized_woa_true, X_train_tfidf, y_train)
mlp_false_fine_tuned = fine_tune_mlp_pso(mlp_optimized_woa_false, X_train_tfidf, 1 - y_train)

# Custom training loop to capture neutrosophic scores
def train_mlp_neutrosophic(X_train, y_train, mlp_true, mlp_false, max_iter=50):
    train_neutrosophic_scores = []

    for i in range(max_iter):
        T_train = mlp_true.predict_proba(X_train)[:, 1]
        F_train = mlp_false.predict_proba(X_train)[:, 1]

        # Calculate Indeterminacy (I)
        I_train = 1 - T_train - F_train

        train_neutrosophic_scores.append((T_train, I_train, F_train))

    return train_neutrosophic_scores

# Train the models with custom neutrosophic training loop
train_neutrosophic_scores = train_mlp_neutrosophic(X_train_tfidf, y_train, mlp_true_fine_tuned, mlp_false_fine_tuned)

# Extracting loss from neutrosophic scores
train_loss_neutro_true = [log_loss(y_train, T) for T, _, _ in train_neutrosophic_scores]
train_loss_neutro_false = [log_loss(1 - y_train, F) for _, _, F in train_neutrosophic_scores]

# Function to plot training neutrosophic loss curves
def plot_neutrosophic_curves(train_loss_true, train_loss_false):
    plt.figure(figsize=(12, 6))

    # Plot training loss for True Membership Function
    plt.plot(train_loss_true, label='Neutrosophic True Membership Function Training Loss')

    # Plot training loss for False Membership Function
    plt.plot(train_loss_false, label='Neutrosophic False Membership Function Training Loss')

    # Adding labels and legend
    plt.title('Neutrosophic Training Loss Curves')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Plot the training neutrosophic loss curves
plot_neutrosophic_curves(train_loss_neutro_true, train_loss_neutro_false)

#adjust weights
import numpy as np
import pandas as pd
import re
import random
import nltk
from nltk.corpus import wordnet, stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import log_loss
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
from pyswarm import pso

# Ensure NLTK WordNet and stopwords are downloaded
nltk.download('wordnet')
nltk.download('stopwords')

# Define stop words
stop_words = set(stopwords.words('english'))

# Define the clean_tweet function
def clean_tweet(tweet):
    tweet = re.sub(r'http\S+', '', tweet)  # Remove URLs
    tweet = re.sub(r'@\w+', '', tweet)     # Remove mentions
    tweet = re.sub(r'#\w+', '', tweet)     # Remove hashtags
    tweet = re.sub(r'\d+', '', tweet)      # Remove numbers
    tweet = re.sub(r'[^\w\s]', '', tweet)  # Remove punctuation
    tweet = tweet.lower()                  # Convert to lowercase
    tweet = tweet.strip()                  # Remove leading/trailing whitespaces
    return tweet

# Data augmentation functions
def get_synonyms(word):
    synonyms = set()
    for syn in wordnet.synsets(word):
        for l in syn.lemmas():
            synonym = l.name().replace("_", " ").replace("-", " ").lower()
            synonym = "".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])
            synonyms.add(synonym)
    if word in synonyms:
        synonyms.remove(word)
    return list(synonyms)

def synonym_replacement(words, n):
    new_words = words.copy()
    random_word_list = list(set([word for word in words if word not in stop_words]))
    random.shuffle(random_word_list)
    num_replaced = 0
    for random_word in random_word_list:
        synonyms = get_synonyms(random_word)
        if len(synonyms) >= 1:
            synonym = random.choice(list(synonyms))
            new_words = [synonym if word == random_word else word for word in new_words]
            num_replaced += 1
        if num_replaced >= n:
            break

    sentence = ' '.join(new_words)
    new_words = sentence.split(' ')

    return new_words

def random_deletion(words, p):
    if len(words) == 1:
        return words

    new_words = []
    for word in words:
        r = random.uniform(0, 1)
        if r > p:
            new_words.append(word)

    if len(new_words) == 0:
        rand_int = random.randint(0, len(words)-1)
        return [words[rand_int]]

    return new_words

def random_swap(words, n):
    if len(words) < 2:
        return words
    new_words = words.copy()
    for _ in range(n):
        new_words = swap_word(new_words)
    return new_words

def swap_word(new_words):
    random_idx_1 = random.randint(0, len(new_words)-1)
    random_idx_2 = random_idx_1
    counter = 0
    while random_idx_1 == random_idx_2:
        random_idx_2 = random.randint(0, len(new_words)-1)
        counter += 1
        if counter > 3:
            return new_words
    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]
    return new_words

def random_insertion(words, n):
    new_words = words.copy()
    for _ in range(n):
        add_word(new_words)
    return new_words

def add_word(new_words):
    synonyms = []
    counter = 0
    while len(synonyms) < 1:
        random_word = new_words[random.randint(0, len(new_words)-1)]
        synonyms = get_synonyms(random_word)
        counter += 1
        if counter >= 10:
            return
    random_synonym = random.choice(list(synonyms))
    random_idx = random.randint(0, len(new_words)-1)
    new_words.insert(random_idx, random_synonym)

def augment_sentence(sentence):
    words = sentence.split()
    num_words = len(words)
    if num_words == 0:
        return [sentence]

    augmented_sentences = []
    augmented_sentences.append(' '.join(synonym_replacement(words, max(1, int(0.1*num_words)))))
    augmented_sentences.append(' '.join(random_deletion(words, 0.1)))
    augmented_sentences.append(' '.join(random_swap(words, max(1, int(0.1*num_words)))))
    augmented_sentences.append(' '.join(random_insertion(words, max(1, int(0.1*num_words)))))

    return augmented_sentences

# Load the dataset
file_path = '/content/labeled_data.csv'
data = pd.read_csv(file_path)

# Remove unnecessary columns and clean the data
data = data[['tweet', 'class']]
data['tweet'] = data['tweet'].apply(clean_tweet)

# Encode the labels
label_encoder = LabelEncoder()
data['class'] = label_encoder.fit_transform(data['class'])

# Data augmentation
augmented_data = []

for index, row in data.iterrows():
    tweet = row['tweet']
    label = row['class']
    augmented_sentences = augment_sentence(tweet)
    for augmented_sentence in augmented_sentences:
        augmented_data.append((augmented_sentence, label))

augmented_df = pd.DataFrame(augmented_data, columns=['tweet', 'class'])
data = pd.concat([data, augmented_df])

# Split the data
X_train, X_test, y_train, y_test = train_test_split(data['tweet'], data['class'], test_size=0.2, random_state=42)

# Vectorize the tweet text using TF-IDF
vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Whale Optimization Algorithm (WOA) implementation
def woa(objective, bounds, num_whales=30, max_iter=50):
    dim = len(bounds)
    lb = np.array([b[0] for b in bounds])
    ub = np.array([b[1] for b in bounds])

    # Initialize the whales' positions
    whales = lb + (ub - lb) * np.random.rand(num_whales, dim)
    fitness = np.array([objective(w) for w in whales])
    best_idx = np.argmin(fitness)
    best_whale = whales[best_idx]
    best_fitness = fitness[best_idx]

    for t in range(max_iter):
        a = 2 - 2 * t / max_iter
        a2 = -1 + t * (-1) / max_iter

        for i in range(num_whales):
            r1, r2 = np.random.rand(), np.random.rand()
            A = 2 * a * r1 - a
            C = 2 * r2
            b = 1
            l = (a2 - 1) * np.random.rand() + 1

            p = np.random.rand()

            if p < 0.5:
                if abs(A) < 1:
                    D = abs(C * best_whale - whales[i])
                    whales[i] = best_whale - A * D
                else:
                    random_whale = whales[np.random.randint(0, num_whales)]
                    D = abs(C * random_whale - whales[i])
                    whales[i] = random_whale - A * D
            else:
                distance_to_best = abs(best_whale - whales[i])
                whales[i] = distance_to_best * np.exp(b * l) * np.cos(l * 2 * np.pi) + best_whale

            whales[i] = np.clip(whales[i], lb, ub)
            fitness[i] = objective(whales[i])

        best_idx = np.argmin(fitness)
        if fitness[best_idx] < best_fitness:
            best_fitness = fitness[best_idx]
            best_whale = whales[best_idx]

    return best_whale, best_fitness

# Optimize MLP weights using WOA
def optimize_mlp_weights_woa(X_train, y_train, random_state=42):
    mlp = MLPClassifier(max_iter=50, random_state=random_state)

    def objective(params):
        # Reshape the flattened parameters into their original shapes
        n_features = X_train.shape[1]
        n_classes = len(np.unique(y_train))
        n_params = n_features * mlp.hidden_layer_sizes[0] + mlp.hidden_layer_sizes[0] * n_classes + mlp.hidden_layer_sizes[0] + n_classes
        params = params.reshape(-1, 1)
        weights1 = params[:n_features * mlp.hidden_layer_sizes[0]].reshape((n_features, mlp.hidden_layer_sizes[0]))
        params = params[n_features * mlp.hidden_layer_sizes[0]:]
        biases1 = params[:mlp.hidden_layer_sizes[0]].reshape((1, -1))
        params = params[mlp.hidden_layer_sizes[0]:]
        weights2 = params[:mlp.hidden_layer_sizes[0] * n_classes].reshape((mlp.hidden_layer_sizes[0], n_classes))
        params = params[mlp.hidden_layer_sizes[0] * n_classes:]
        biases2 = params[:n_classes].reshape((1, -1))

        # Update the MLP with the new weights and biases
        mlp.coefs_ = [weights1, weights2]
        mlp.intercepts_ = [biases1, biases2]

        # Train the MLP and calculate the log loss
        mlp.fit(X_train, y_train)
        return log_loss(y_train, mlp.predict_proba(X_train))

    # Flatten the initial weights and biases matrices into a single vector
    n_features = X_train.shape[1]
    n_classes = len(np.unique(y_train))
    n_params = n_features * mlp.hidden_layer_sizes[0] + mlp.hidden_layer_sizes[0] * n_classes + mlp.hidden_layer_sizes[0] + n_classes
    initial_params = np.random.uniform(-1, 1, size=n_params)

    # Set bounds for the parameters (e.g., weight values between -1 and 1)
    bounds = [(-1, 1)] * n_params

    # Optimize the parameters using WOA
    best_params, _ = woa(objective, bounds)

    # Reshape the flattened best_params into their original shapes
    best_params = best_params.reshape(-1, 1)
    weights1 = best_params[:n_features * mlp.hidden_layer_sizes[0]].reshape((n_features, mlp.hidden_layer_sizes[0]))
    best_params = best_params[n_features * mlp.hidden_layer_sizes[0]:]
    biases1 = best_params[:mlp.hidden_layer_sizes[0]].reshape((1, -1))
    best_params = best_params[mlp.hidden_layer_sizes[0]:]
    weights2 = best_params[:mlp.hidden_layer_sizes[0] * n_classes].reshape((mlp.hidden_layer_sizes[0], n_classes))
    best_params = best_params[mlp.hidden_layer_sizes[0] * n_classes:]
    biases2 = best_params[:n_classes].reshape((1, -1))

    # Update the MLP with the best weights and biases
    mlp.coefs_ = [weights1, weights2]
    mlp.intercepts_ = [biases1, biases2]

    # Train the MLP with the best parameters
    mlp.fit(X_train, y_train)

    return mlp



# Optimize MLP using WOA
mlp_optimized_woa_true = optimize_mlp_weights_woa(X_train_tfidf, y_train)
mlp_optimized_woa_false = optimize_mlp_weights_woa(X_train_tfidf, 1 - y_train)

# Particle Swarm Optimization (PSO) for fine-tuning MLP
def fine_tune_mlp_pso(mlp, X_train, y_train):
    def objective(params):
        mlp.set_params(hidden_layer_sizes=(int(params[0]),))
        mlp.fit(X_train, y_train)
        return log_loss(y_train, mlp.predict_proba(X_train))

    bounds = [(50, 500)]  # Hidden layer sizes

    best_params, _ = pso(objective, *zip(*bounds))

    mlp.set_params(hidden_layer_sizes=(int(best_params[0]),))
    mlp.fit(X_train, y_train)

    return mlp

# Fine-tune MLPs using PSO
mlp_true_fine_tuned = fine_tune_mlp_pso(mlp_optimized_woa_true, X_train_tfidf, y_train)
mlp_false_fine_tuned = fine_tune_mlp_pso(mlp_optimized_woa_false, X_train_tfidf, 1 - y_train)

# Custom training loop to capture neutrosophic scores
def train_mlp_neutrosophic(X_train, y_train, mlp_true, mlp_false, max_iter=50):
    train_neutrosophic_scores = []

    for i in range(max_iter):
        T_train = mlp_true.predict_proba(X_train)[:, 1]
        F_train = mlp_false.predict_proba(X_train)[:, 1]

        # Calculate Indeterminacy (I)
        I_train = 1 - T_train - F_train

        train_neutrosophic_scores.append((T_train, I_train, F_train))

    return train_neutrosophic_scores

# Train the models with custom neutrosophic training loop
train_neutrosophic_scores = train_mlp_neutrosophic(X_train_tfidf, y_train, mlp_true_fine_tuned, mlp_false_fine_tuned)

# Extracting loss from neutrosophic scores
train_loss_neutro_true = [log_loss(y_train, T) for T, _, _ in train_neutrosophic_scores]
train_loss_neutro_false = [log_loss(1 - y_train, F) for _, _, F in train_neutrosophic_scores]

# Function to plot training neutrosophic loss curves
def plot_neutrosophic_curves(train_loss_true, train_loss_false):
    plt.figure(figsize=(12, 6))

    # Plot training loss for True Membership Function
    plt.plot(train_loss_true, label='Neutrosophic True Membership Function Training Loss')

    # Plot training loss for False Membership Function
    plt.plot(train_loss_false, label='Neutrosophic False Membership Function Training Loss')

    # Adding labels and legend
    plt.title('Neutrosophic Training Loss Curves')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Plot the training neutrosophic loss curves
plot_neutrosophic_curves(train_loss_neutro_true, train_loss_neutro_false)