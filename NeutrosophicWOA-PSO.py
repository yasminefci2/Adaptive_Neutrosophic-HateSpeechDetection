# -*- coding: utf-8 -*-
"""Neutrosophic18-5-2024.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ahFa_71pRTPdpH6xNsZZgvP-1qU-D3ej

We use the Whale Optimization Algorithm (WOA) to initially adjust the weights of the MLP.
We then use the Particle Swarm Optimization (PSO) algorithm to fine-tune the weights starting from the best weights found by WOA.
This two-step optimization process first uses WOA for a broad search and then PSO for more refined fine-tuning.
"""

import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

class WhaleOptimizationAlgorithm:
    def __init__(self, num_dimensions, lower_bound, upper_bound, population_size=10, max_iterations=100):
        self.num_dimensions = num_dimensions
        self.lower_bound = lower_bound
        self.upper_bound = upper_bound
        self.population_size = population_size
        self.max_iterations = max_iterations

    def initialize_population(self):
        return np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.num_dimensions))

    def optimize(self, fitness_function):
        population = self.initialize_population()
        fitness_values = np.array([fitness_function(individual) for individual in population])
        best_fitness_idx = np.argmin(fitness_values)
        best_solution = population[best_fitness_idx].copy()
        best_fitness = fitness_values[best_fitness_idx]

        for iteration in range(self.max_iterations):
            a = 2 - 2 * iteration / self.max_iterations  # linearly decreases from 2 to 0

            for i, individual in enumerate(population):
                A = 2 * a * np.random.rand(self.num_dimensions) - a
                C = 2 * np.random.rand(self.num_dimensions)
                l = np.random.uniform(-1, 1)
                p = np.random.rand()

                if p < 0.5:
                    if np.all(np.abs(A) < 1):
                        D = np.abs(C * best_solution - individual)
                        new_individual = best_solution - A * D
                    else:
                        random_individual_idx = np.random.randint(0, self.population_size)
                        random_individual = population[random_individual_idx]
                        D = np.abs(C * random_individual - individual)
                        new_individual = random_individual - A * D
                else:
                    new_individual = np.abs(best_solution - individual) * np.exp(l) * np.cos(2 * np.pi * l) + best_solution

                new_individual = np.clip(new_individual, self.lower_bound, self.upper_bound)
                fitness = fitness_function(new_individual)

                if fitness < best_fitness:
                    best_fitness = fitness
                    best_solution = new_individual.copy()

                population[i] = new_individual

        return best_solution

class ParticleSwarmOptimization:
    def __init__(self, num_dimensions, lower_bound, upper_bound, population_size=10, max_iterations=100, w=0.5, c1=1.5, c2=1.5):
        self.num_dimensions = num_dimensions
        self.lower_bound = lower_bound
        self.upper_bound = upper_bound
        self.population_size = population_size
        self.max_iterations = max_iterations
        self.w = w  # inertia weight
        self.c1 = c1  # cognitive component
        self.c2 = c2  # social component

    def initialize_population(self):
        particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.num_dimensions))
        velocities = np.random.uniform(-1, 1, (self.population_size, self.num_dimensions))
        return particles, velocities

    def optimize(self, fitness_function, initial_solution):
        particles, velocities = self.initialize_population()
        particles[0] = initial_solution  # Initialize with the best solution from WOA
        personal_best_positions = particles.copy()
        personal_best_scores = np.array([fitness_function(p) for p in particles])
        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]
        global_best_score = np.min(personal_best_scores)

        for iteration in range(self.max_iterations):
            for i in range(self.population_size):
                r1, r2 = np.random.rand(2, self.num_dimensions)
                velocities[i] = (
                    self.w * velocities[i] +
                    self.c1 * r1 * (personal_best_positions[i] - particles[i]) +
                    self.c2 * r2 * (global_best_position - particles[i])
                )
                particles[i] += velocities[i]
                particles[i] = np.clip(particles[i], self.lower_bound, self.upper_bound)

                score = fitness_function(particles[i])
                if score < personal_best_scores[i]:
                    personal_best_scores[i] = score
                    personal_best_positions[i] = particles[i].copy()

                if score < global_best_score:
                    global_best_score = score
                    global_best_position = particles[i].copy()

        return global_best_position

# Load the labeled data
data = pd.read_csv('labeled_data.csv')

# Preprocess the data
X = data['tweet']
y = data['class']

# Convert text data to numerical features using CountVectorizer
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the MLPClassifier model for Truth MLP
model_truth = MLPClassifier(hidden_layer_sizes=(50,), max_iter=100)

# Fit the model to get the weights
model_truth.fit(X_train, y_train)

# Get the number of dimensions (total number of weights in MLP)
num_dimensions = sum((layer.size for layer in model_truth.coefs_))

# Define the bounds for weights
lower_bound = -1
upper_bound = 1

# Initialize WOA
woa = WhaleOptimizationAlgorithm(num_dimensions, lower_bound, upper_bound)

# Define the fitness function for WOA
def fitness_function_woa(weights):
    # Reshape the weights to match MLP model's shape
    start = 0
    new_weights = []
    for layer in model_truth.coefs_:
        end = start + layer.size
        new_weights.append(weights[start:end].reshape(layer.shape))
        start = end
    model_truth.coefs_ = new_weights

    # Train the model with current weights
    model_truth.fit(X_train, y_train)

    # Evaluate the model
    y_pred = model_truth.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    return -accuracy  # Negative since WOA minimizes

# Optimize the weights using WOA
best_weights_woa = woa.optimize(fitness_function_woa)

# Initialize PSO
pso = ParticleSwarmOptimization(num_dimensions, lower_bound, upper_bound)

# Define the fitness function for PSO
def fitness_function_pso(weights):
    # Reshape the weights to match MLP model's shape
    start = 0
    new_weights = []
    for layer in model_truth.coefs_:
        end = start + layer.size
        new_weights.append(weights[start:end].reshape(layer.shape))
        start = end
    model_truth.coefs_ = new_weights

    # Train the model with current weights
    model_truth.fit(X_train, y_train)

    # Evaluate the model
    y_pred = model_truth.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    return -accuracy  # Negative since PSO minimizes

# Fine-tune the weights using PSO starting from the best weights found by WOA
best_weights_pso = pso.optimize(fitness_function_pso, best_weights_woa)

# Reshape the best weights to match MLP model's shape
start = 0
new_weights = []
for layer in model_truth.coefs_:
    end = start + layer.size
    new_weights.append(best_weights_pso[start:end].reshape(layer.shape))
    start = end

# Set the optimized weights to the Truth MLP model
model_truth.coefs_ = new_weights

# Train the model with optimized weights
model_truth.fit(X_train, y_train)

# Define the complement of the target codewords for Falsity MLP
y_complement = 1 - pd.get_dummies(y).values

# Define the MLPClassifier model for Falsity MLP
model_falsity = MLPClassifier(hidden_layer_sizes=(50,), max_iter=100)

# Train the Falsity MLP with the complement codewords
model_falsity.fit(X_train, y_complement)

# Reshape and assign the best weights found by WOA to the Falsity MLP for initialization
start = 0
new_weights = []
for layer in model_falsity.coefs_:
    end = start + layer.size
    new_weights.append(best_weights_pso[start:end].reshape(layer.shape))
    start = end
model_falsity.coefs_ = new_weights

# Fine-tune the Falsity MLP with PSO
best_weights_pso_falsity = pso.optimize(fitness_function_pso, best_weights_pso)

# Reshape the best weights to match Falsity MLP model's shape
start = 0
new_weights = []
for layer in model_falsity.coefs_:
    end = start + layer.size
    new_weights.append(best_weights_pso_falsity[start:end].reshape(layer.shape))
    start = end

# Set the optimized weights to the Falsity MLP model
model_falsity.coefs_ = new_weights

# Train the Falsity MLP with optimized weights
model_falsity.fit(X_train, y_complement)

# Function to predict using neutrosophic logic
def neutrosophic_predict(X):
    T = model_truth.predict_proba(X)
    F = model_falsity.predict_proba(X)
    I = 1 - np.abs(T - F)
    A = np.stack([T, I, F], axis=2)
    return A

# Predict with neutrosophic logic
neutrosophic_results = neutrosophic_predict(X_test)

# Example to show neutrosophic results for the first sample
print("Truth Memberships:", neutrosophic_results[0,:,0])
print("Indeterminacy Memberships:", neutrosophic_results[0,:,1])
print("Falsity Memberships:", neutrosophic_results[0,:,2])

#After Decision
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Define the Whale Optimization Algorithm class
class WhaleOptimizationAlgorithm:
    def __init__(self, num_dimensions, lower_bound, upper_bound, population_size=10, max_iterations=50):
        self.num_dimensions = num_dimensions
        self.lower_bound = lower_bound
        self.upper_bound = upper_bound
        self.population_size = population_size
        self.max_iterations = max_iterations

    def initialize_population(self):
        return np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.num_dimensions))

    def optimize(self, fitness_function):
        population = self.initialize_population()
        fitness_values = np.array([fitness_function(individual) for individual in population])
        best_fitness_idx = np.argmin(fitness_values)
        best_solution = population[best_fitness_idx].copy()
        best_fitness = fitness_values[best_fitness_idx]

        for iteration in range(self.max_iterations):
            a = 2 - 2 * iteration / self.max_iterations  # linearly decreases from 2 to 0

            for i, individual in enumerate(population):
                A = 2 * a * np.random.rand(self.num_dimensions) - a
                C = 2 * np.random.rand(self.num_dimensions)
                l = np.random.uniform(-1, 1)
                p = np.random.rand()

                if p < 0.5:
                    if np.all(np.abs(A) < 1):
                        D = np.abs(C * best_solution - individual)
                        new_individual = best_solution - A * D
                    else:
                        random_individual_idx = np.random.randint(0, self.population_size)
                        random_individual = population[random_individual_idx]
                        D = np.abs(C * random_individual - individual)
                        new_individual = random_individual - A * D
                else:
                    new_individual = np.abs(best_solution - individual) * np.exp(l) * np.cos(2 * np.pi * l) + best_solution

                new_individual = np.clip(new_individual, self.lower_bound, self.upper_bound)
                fitness = fitness_function(new_individual)

                if fitness < best_fitness:
                    best_fitness = fitness
                    best_solution = new_individual.copy()

                population[i] = new_individual

        return best_solution

# Define the Particle Swarm Optimization class
class ParticleSwarmOptimization:
    def __init__(self, num_dimensions, lower_bound, upper_bound, population_size=10, max_iterations=50, w=0.5, c1=1.5, c2=1.5):
        self.num_dimensions = num_dimensions
        self.lower_bound = lower_bound
        self.upper_bound = upper_bound
        self.population_size = population_size
        self.max_iterations = max_iterations
        self.w = w  # inertia weight
        self.c1 = c1  # cognitive component
        self.c2 = c2  # social component

    def initialize_population(self):
        particles = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.num_dimensions))
        velocities = np.random.uniform(-1, 1, (self.population_size, self.num_dimensions))
        return particles, velocities

    def optimize(self, fitness_function, initial_solution):
        particles, velocities = self.initialize_population()
        particles[0] = initial_solution  # Initialize with the best solution from WOA
        personal_best_positions = particles.copy()
        personal_best_scores = np.array([fitness_function(p) for p in particles])
        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]
        global_best_score = np.min(personal_best_scores)

        for iteration in range(self.max_iterations):
            for i in range(self.population_size):
                r1, r2 = np.random.rand(2, self.num_dimensions)
                velocities[i] = (
                    self.w * velocities[i] +
                    self.c1 * r1 * (personal_best_positions[i] - particles[i]) +
                    self.c2 * r2 * (global_best_position - particles[i])
                )
                particles[i] += velocities[i]
                particles[i] = np.clip(particles[i], self.lower_bound, self.upper_bound)

                score = fitness_function(particles[i])
                if score < personal_best_scores[i]:
                    personal_best_scores[i] = score
                    personal_best_positions[i] = particles[i].copy()

                if score < global_best_score:
                    global_best_score = score
                    global_best_position = particles[i].copy()

        return global_best_position

# Load the labeled data
data = pd.read_csv('labeled_data.csv')

# Preprocess the data
X = data['tweet']
y = data['class']

# Convert text data to numerical features using CountVectorizer
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the MLPClassifier model for Truth MLP
model_truth = MLPClassifier(hidden_layer_sizes=(50,), max_iter=50)

# Fit the model to get the weights
model_truth.fit(X_train, y_train)

# Get the number of dimensions (total number of weights in MLP)
num_dimensions = sum((layer.size for layer in model_truth.coefs_))

# Define the bounds for weights
lower_bound = -1
upper_bound = 1

# Initialize WOA
woa = WhaleOptimizationAlgorithm(num_dimensions, lower_bound, upper_bound)

# Define the fitness function for WOA
def fitness_function_woa(weights):
    # Reshape the weights to match MLP model's shape
    start = 0
    new_weights = []
    for layer in model_truth.coefs_:
        end = start + layer.size
        new_weights.append(weights[start:end].reshape(layer.shape))
        start = end
    model_truth.coefs_ = new_weights

    # Train the model with current weights
    model_truth.fit(X_train, y_train)

    # Evaluate the model
    y_pred = model_truth.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    return -accuracy  # Negative since WOA minimizes

# Optimize the weights using WOA
best_weights_woa = woa.optimize(fitness_function_woa)

# Initialize PSO
pso = ParticleSwarmOptimization(num_dimensions, lower_bound, upper_bound)

# Define the fitness function for PSO
def fitness_function_pso(weights):
    # Reshape the weights to match MLP model's shape
    start = 0
    new_weights = []
    for layer in model_truth.coefs_:
        end = start + layer.size
        new_weights.append(weights[start:end].reshape(layer.shape))
        start = end
    model_truth.coefs_ = new_weights

    # Train the model with current weights
    model_truth.fit(X_train, y_train)

    # Evaluate the model
    y_pred = model_truth.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    return -accuracy  # Negative since PSO minimizes

# Fine-tune the weights using PSO starting from the best weights found by WOA
best_weights_pso = pso.optimize(fitness_function_pso, best_weights_woa)

# Reshape the best weights to match MLP model's shape
start = 0
new_weights = []
for layer in model_truth.coefs_:
    end = start + layer.size
    new_weights.append(best_weights_pso[start:end].reshape(layer.shape))
    start = end

# Set the optimized weights to the Truth MLP model
model_truth.coefs_ = new_weights

# Train the model with optimized weights
model_truth.fit(X_train, y_train)

# Define the complement of the target codewords for Falsity MLP
y_complement = 1 - pd.get_dummies(y).values

# Define the MLPClassifier model for Falsity MLP
model_falsity = MLPClassifier(hidden_layer_sizes=(50,), max_iter=100)

# Train the Falsity MLP with the complement codewords
model_falsity.fit(X_train, y_complement)

# Reshape and assign the best weights found by WOA to the Falsity MLP for initialization
start = 0
new_weights = []
for layer in model_falsity.coefs_:
    end = start + layer.size
    new_weights.append(best_weights_pso[start:end].reshape(layer.shape))
    start = end
model_falsity.coefs_ = new_weights

# Fine-tune the Falsity MLP with PSO
best_weights_pso_falsity = pso.optimize(fitness_function_pso, best_weights_pso)

# Reshape the best weights to match Falsity MLP model's shape
start = 0
new_weights = []
for layer in model_falsity.coefs_:
    end = start + layer.size
    new_weights.append(best_weights_pso_falsity[start:end].reshape(layer.shape))
    start = end

# Set the optimized weights to the Falsity MLP model
model_falsity.coefs_ = new_weights

# Train the Falsity MLP with optimized weights
model_falsity.fit(X_train, y_complement)

# Function to predict using neutrosophic logic
def neutrosophic_predict(X):
    T = model_truth.predict_proba(X)
    F = model_falsity.predict_proba(X)
    I = 1 - np.abs(T - F)
    A = np.stack([T, I, F], axis=2)
    return A

# Predict with neutrosophic logic
neutrosophic_results = neutrosophic_predict(X_test)

# Decision making based on neutrosophic sets
def make_decision(neutrosophic_results, indeterminacy_threshold=0.5):
    decisions = []
    for result in neutrosophic_results:
        T, I, F = result[:, 0], result[:, 1], result[:, 2]
        if np.min(I) > indeterminacy_threshold:
            # If indeterminacy is high for all classes, abstain or choose an alternative decision strategy
            decisions.append(-1)  # -1 indicates abstention
        else:
            # Otherwise, choose the class with the highest truth membership
            decisions.append(np.argmax(T))
    return np.array(decisions)

# Make decisions for the test set
decisions = make_decision(neutrosophic_results)

# Evaluate the decisions
accuracy = accuracy_score(y_test, decisions[decisions != -1])  # Exclude abstentions
print("Accuracy with neutrosophic decision making:", accuracy)