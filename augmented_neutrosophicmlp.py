# -*- coding: utf-8 -*-
"""augmented neutrosophicMLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vssBjZw5I6EezlI9U1xcsa35vVSFFBby
"""

import numpy as np
import pandas as pd
import re
import random
import nltk
from nltk.corpus import wordnet, stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt

# Ensure NLTK WordNet and stopwords are downloaded
nltk.download('wordnet')
nltk.download('stopwords')

# Define stop words
stop_words = set(stopwords.words('english'))

# Define the clean_tweet function
def clean_tweet(tweet):
    tweet = re.sub(r'http\S+', '', tweet)  # Remove URLs
    tweet = re.sub(r'@\w+', '', tweet)     # Remove mentions
    tweet = re.sub(r'#\w+', '', tweet)     # Remove hashtags
    tweet = re.sub(r'\d+', '', tweet)      # Remove numbers
    tweet = re.sub(r'[^\w\s]', '', tweet)  # Remove punctuation
    tweet = tweet.lower()                  # Convert to lowercase
    tweet = tweet.strip()                  # Remove leading/trailing whitespaces
    return tweet

# Data augmentation functions
def get_synonyms(word):
    synonyms = set()
    for syn in wordnet.synsets(word):
        for l in syn.lemmas():
            synonym = l.name().replace("_", " ").replace("-", " ").lower()
            synonym = "".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])
            synonyms.add(synonym)
    if word in synonyms:
        synonyms.remove(word)
    return list(synonyms)

def synonym_replacement(words, n):
    new_words = words.copy()
    random_word_list = list(set([word for word in words if word not in stop_words]))
    random.shuffle(random_word_list)
    num_replaced = 0
    for random_word in random_word_list:
        synonyms = get_synonyms(random_word)
        if len(synonyms) >= 1:
            synonym = random.choice(list(synonyms))
            new_words = [synonym if word == random_word else word for word in new_words]
            num_replaced += 1
        if num_replaced >= n:
            break

    sentence = ' '.join(new_words)
    new_words = sentence.split(' ')

    return new_words

def random_deletion(words, p):
    if len(words) == 1:
        return words

    new_words = []
    for word in words:
        r = random.uniform(0, 1)
        if r > p:
            new_words.append(word)

    if len(new_words) == 0:
        rand_int = random.randint(0, len(words)-1)
        return [words[rand_int]]

    return new_words

def random_swap(words, n):
    if len(words) < 2:
        return words
    new_words = words.copy()
    for _ in range(n):
        new_words = swap_word(new_words)
    return new_words

def swap_word(new_words):
    random_idx_1 = random.randint(0, len(new_words)-1)
    random_idx_2 = random_idx_1
    counter = 0
    while random_idx_1 == random_idx_2:
        random_idx_2 = random.randint(0, len(new_words)-1)
        counter += 1
        if counter > 3:
            return new_words
    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]
    return new_words

def random_insertion(words, n):
    new_words = words.copy()
    for _ in range(n):
        add_word(new_words)
    return new_words

def add_word(new_words):
    synonyms = []
    counter = 0
    while len(synonyms) < 1:
        random_word = new_words[random.randint(0, len(new_words)-1)]
        synonyms = get_synonyms(random_word)
        counter += 1
        if counter >= 10:
            return
    random_synonym = random.choice(list(synonyms))
    random_idx = random.randint(0, len(new_words)-1)
    new_words.insert(random_idx, random_synonym)

def augment_sentence(sentence):
    words = sentence.split()
    num_words = len(words)
    if num_words == 0:
        return [sentence]

    augmented_sentences = []
    augmented_sentences.append(' '.join(synonym_replacement(words, max(1, int(0.1*num_words)))))
    augmented_sentences.append(' '.join(random_deletion(words, 0.1)))
    augmented_sentences.append(' '.join(random_swap(words, max(1, int(0.1*num_words)))))
    augmented_sentences.append(' '.join(random_insertion(words, max(1, int(0.1*num_words)))))

    return augmented_sentences

# Load the dataset
file_path = '/content/labeled_data.csv'
data = pd.read_csv(file_path)

# Remove unnecessary columns and clean the data
data = data[['tweet', 'class']]
data['tweet'] = data['tweet'].apply(clean_tweet)

# Encode the labels
label_encoder = LabelEncoder()
data['class'] = label_encoder.fit_transform(data['class'])

# Data augmentation
augmented_data = []

for index, row in data.iterrows():
    tweet = row['tweet']
    label = row['class']
    augmented_sentences = augment_sentence(tweet)
    for augmented_sentence in augmented_sentences:
        augmented_data.append((augmented_sentence, label))

augmented_df = pd.DataFrame(augmented_data, columns=['tweet', 'class'])
data = pd.concat([data, augmented_df])

# Split the data
X_train, X_test, y_train, y_test = train_test_split(data['tweet'], data['class'], test_size=0.2, random_state=42)

# Vectorize the tweet text using TF-IDF
vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Define and train the MLP for True Membership Function (T)
mlp_true = MLPClassifier(hidden_layer_sizes=(100,), max_iter=100, activation='relu', solver='adam', random_state=42)
history_true = mlp_true.fit(X_train_tfidf, y_train)

# Generate complementary labels for False Membership Function (F)
num_classes = len(np.unique(y_train))

y_train_false = np.zeros_like(y_train)
y_test_false = np.zeros_like(y_test)

for i in range(len(y_train)):
    y_train_false[i] = num_classes - 1 - y_train.values[i]
for i in range(len(y_test)):
    y_test_false[i] = num_classes - 1 - y_test.values[i]

# Define and train the MLP for False Membership Function (F)
mlp_false = MLPClassifier(hidden_layer_sizes=(100,), max_iter=100, activation='relu', solver='adam', random_state=42)
history_false = mlp_false.fit(X_train_tfidf, y_train_false)

# Predict using both MLPs
T = mlp_true.predict_proba(X_test_tfidf)
F = mlp_false.predict_proba(X_test_tfidf)

# Calculate Indeterminacy (I)
I = 1 - T - F

# Evaluate the models
y_pred_true = mlp_true.predict(X_test_tfidf)
y_pred_false = mlp_false.predict(X_test_tfidf)

# Calculate the final prediction based on T, F, and I (simplified approach)
y_pred_final = np.argmax(T, axis=1)

accuracy = accuracy_score(y_test, y_pred_final)
report = classification_report(y_test, y_pred_final, target_names=label_encoder.classes_.astype(str))

print(f"Accuracy: {accuracy}")
print(f"Classification Report:\n{report}")

# Plotting the training and test curves
plt.figure(figsize=(12, 6))

# Training loss
plt.plot(history_true.loss_curve_, label='True Membership Function Training Loss')
plt.plot(history_false.loss_curve_, label='False Membership Function Training Loss')

# Adding labels and legend
plt.title('Training Loss Curve')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Predicting a list of tweets
tweets = ["I hate this!", "This is offensive!", "Have a nice day!"]
cleaned_tweets = [clean_tweet(tweet) for tweet in tweets]
tweets_tfidf = vectorizer.transform(cleaned_tweets)

predictions = mlp_true.predict(tweets_tfidf)
predicted_classes = label_encoder.inverse_transform(predictions)

for tweet, predicted_class in zip(tweets, predicted_classes):
    print(f"Tweet: {tweet} -> Predicted Class: {predicted_class}")

import numpy as np
import pandas as pd
import re
import random
import nltk
from nltk.corpus import wordnet, stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, accuracy_score, log_loss
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt

# Ensure NLTK WordNet and stopwords are downloaded
nltk.download('wordnet')
nltk.download('stopwords')

# Define stop words
stop_words = set(stopwords.words('english'))

# Define the clean_tweet function
def clean_tweet(tweet):
    tweet = re.sub(r'http\S+', '', tweet)  # Remove URLs
    tweet = re.sub(r'@\w+', '', tweet)     # Remove mentions
    tweet = re.sub(r'#\w+', '', tweet)     # Remove hashtags
    tweet = re.sub(r'\d+', '', tweet)      # Remove numbers
    tweet = re.sub(r'[^\w\s]', '', tweet)  # Remove punctuation
    tweet = tweet.lower()                  # Convert to lowercase
    tweet = tweet.strip()                  # Remove leading/trailing whitespaces
    return tweet

# Data augmentation functions
def get_synonyms(word):
    synonyms = set()
    for syn in wordnet.synsets(word):
        for l in syn.lemmas():
            synonym = l.name().replace("_", " ").replace("-", " ").lower()
            synonym = "".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])
            synonyms.add(synonym)
    if word in synonyms:
        synonyms.remove(word)
    return list(synonyms)

def synonym_replacement(words, n):
    new_words = words.copy()
    random_word_list = list(set([word for word in words if word not in stop_words]))
    random.shuffle(random_word_list)
    num_replaced = 0
    for random_word in random_word_list:
        synonyms = get_synonyms(random_word)
        if len(synonyms) >= 1:
            synonym = random.choice(list(synonyms))
            new_words = [synonym if word == random_word else word for word in new_words]
            num_replaced += 1
        if num_replaced >= n:
            break

    sentence = ' '.join(new_words)
    new_words = sentence.split(' ')

    return new_words

def random_deletion(words, p):
    if len(words) == 1:
        return words

    new_words = []
    for word in words:
        r = random.uniform(0, 1)
        if r > p:
            new_words.append(word)

    if len(new_words) == 0:
        rand_int = random.randint(0, len(words)-1)
        return [words[rand_int]]

    return new_words

def random_swap(words, n):
    if len(words) < 2:
        return words
    new_words = words.copy()
    for _ in range(n):
        new_words = swap_word(new_words)
    return new_words

def swap_word(new_words):
    random_idx_1 = random.randint(0, len(new_words)-1)
    random_idx_2 = random_idx_1
    counter = 0
    while random_idx_1 == random_idx_2:
        random_idx_2 = random.randint(0, len(new_words)-1)
        counter += 1
        if counter > 3:
            return new_words
    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]
    return new_words

def random_insertion(words, n):
    new_words = words.copy()
    for _ in range(n):
        add_word(new_words)
    return new_words

def add_word(new_words):
    synonyms = []
    counter = 0
    while len(synonyms) < 1:
        random_word = new_words[random.randint(0, len(new_words)-1)]
        synonyms = get_synonyms(random_word)
        counter += 1
        if counter >= 10:
            return
    random_synonym = random.choice(list(synonyms))
    random_idx = random.randint(0, len(new_words)-1)
    new_words.insert(random_idx, random_synonym)

def augment_sentence(sentence):
    words = sentence.split()
    num_words = len(words)
    if num_words == 0:
        return [sentence]

    augmented_sentences = []
    augmented_sentences.append(' '.join(synonym_replacement(words, max(1, int(0.1*num_words)))))
    augmented_sentences.append(' '.join(random_deletion(words, 0.1)))
    augmented_sentences.append(' '.join(random_swap(words, max(1, int(0.1*num_words)))))
    augmented_sentences.append(' '.join(random_insertion(words, max(1, int(0.1*num_words)))))

    return augmented_sentences

# Load the dataset
file_path = '/content/labeled_data.csv'
data = pd.read_csv(file_path)

# Remove unnecessary columns and clean the data
data = data[['tweet', 'class']]
data['tweet'] = data['tweet'].apply(clean_tweet)

# Encode the labels
label_encoder = LabelEncoder()
data['class'] = label_encoder.fit_transform(data['class'])

# Data augmentation
augmented_data = []

for index, row in data.iterrows():
    tweet = row['tweet']
    label = row['class']
    augmented_sentences = augment_sentence(tweet)
    for augmented_sentence in augmented_sentences:
        augmented_data.append((augmented_sentence, label))

augmented_df = pd.DataFrame(augmented_data, columns=['tweet', 'class'])
data = pd.concat([data, augmented_df])

# Split the data
X_train, X_test, y_train, y_test = train_test_split(data['tweet'], data['class'], test_size=0.2, random_state=42)

# Vectorize the tweet text using TF-IDF
vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Custom training loop to capture validation loss
def train_mlp_with_validation(X_train, y_train, X_val, y_val, max_iter=100, hidden_layer_sizes=(100,), activation='relu', solver='adam', random_state=42):
    mlp = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, max_iter=1, activation=activation, solver=solver, warm_start=True, random_state=random_state)
    train_loss = []
    val_loss = []
    for i in range(max_iter):
        mlp.fit(X_train, y_train)
        train_loss.append(mlp.loss_)
        val_pred = mlp.predict_proba(X_val)
        val_loss.append(log_loss(y_val, val_pred))
    return mlp, train_loss, val_loss

# Split the training data into training and validation sets
X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train_tfidf, y_train, test_size=0.1, random_state=42)

# Train the models with custom validation tracking
mlp_true, train_loss_true, val_loss_true = train_mlp_with_validation(X_train_split, y_train_split, X_val_split, y_val_split)
# Generate complementary labels for False Membership Function (F)
num_classes = len(np.unique(y_train))

y_train_false = np.zeros_like(y_train)
y_test_false = np.zeros_like(y_test)

for i in range(len(y_train)):
    y_train_false[i] = num_classes - 1 - y_train.values[i]
for i in range(len(y_test)):
    y_test_false[i] = num_classes - 1 - y_test.values[i]

# Split the data
X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train_tfidf, y_train, test_size=0.1, random_state=42)
X_train_split_false, X_val_split_false, y_train_false_split, y_val_false_split = train_test_split(X_train_tfidf, y_train_false, test_size=0.1, random_state=42)

# Train the models with custom validation tracking
mlp_true, train_loss_true, val_loss_true = train_mlp_with_validation(X_train_split, y_train_split, X_val_split, y_val_split)
mlp_false, train_loss_false, val_loss_false = train_mlp_with_validation(X_train_split_false, y_train_false_split, X_val_split_false, y_val_false_split)

# Predict using both MLPs
T = mlp_true.predict_proba(X_test_tfidf)
F = mlp_false.predict_proba(X_test_tfidf)

# Calculate Indeterminacy (I)
I = 1 - T - F

# Evaluate the models
y_pred_true = mlp_true.predict(X_test_tfidf)
y_pred_false = mlp_false.predict(X_test_tfidf)

# Calculate the final prediction based on T, F, and I (simplified approach)
y_pred_final = np.argmax(T, axis=1)

accuracy = accuracy_score(y_test, y_pred_final)
report = classification_report(y_test, y_pred_final, target_names=label_encoder.classes_.astype(str))

print(f"Accuracy: {accuracy}")
print(f"Classification Report:\n{report}")

# Function to plot training and validation curves
def plot_training_curves(train_loss_true, val_loss_true, train_loss_false, val_loss_false):
    plt.figure(figsize=(12, 6))

    # Plot training and validation loss for True Membership Function
    plt.plot(train_loss_true, label='True Membership Function Training Loss')
    plt.plot(val_loss_true, label='True Membership Function Validation Loss')

    # Plot training and validation loss for False Membership Function
    plt.plot(train_loss_false, label='False Membership Function Training Loss')
    plt.plot(val_loss_false, label='False Membership Function Validation Loss')

    # Adding labels and legend
    plt.title('Training and Validation Loss Curves')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Plot the training and validation curves
plot_training_curves(train_loss_true, val_loss_true, train_loss_false, val_loss_false)

# Function to predict a batch of tweets
def predict_tweets(tweets):
    cleaned_tweets = [clean_tweet(tweet) for tweet in tweets]
    tweets_tfidf = vectorizer.transform(cleaned_tweets)
    T = mlp_true.predict_proba(tweets_tfidf)
    F = mlp_false.predict_proba(tweets_tfidf)
    I = 1 - T - F
    predictions = np.argmax(T, axis=1)
    return predictions

# Predicting a list of tweets using the predict_tweets function
tweets = ["all jews are the same!", "what a dirtsy man!", "Have a nice day!", "What an ugly girl!", "stop fuckking eating"]
predicted_classes = predict_tweets(tweets)

# Convert predicted class indices back to their original labels
predicted_classes_labels = label_encoder.inverse_transform(predicted_classes)

for tweet, predicted_class in zip(tweets, predicted_classes_labels):
    print(f"Tweet: {tweet} -> Predicted Class: {predicted_class}")

import numpy as np
import pandas as pd
import re
import random
import nltk
from nltk.corpus import wordnet, stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, accuracy_score, log_loss
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt

# Ensure NLTK WordNet and stopwords are downloaded
nltk.download('wordnet')
nltk.download('stopwords')

# Define stop words
stop_words = set(stopwords.words('english'))

# Define the clean_tweet function
def clean_tweet(tweet):
    tweet = re.sub(r'http\S+', '', tweet)  # Remove URLs
    tweet = re.sub(r'@\w+', '', tweet)     # Remove mentions
    tweet = re.sub(r'#\w+', '', tweet)     # Remove hashtags
    tweet = re.sub(r'\d+', '', tweet)      # Remove numbers
    tweet = re.sub(r'[^\w\s]', '', tweet)  # Remove punctuation
    tweet = tweet.lower()                  # Convert to lowercase
    tweet = tweet.strip()                  # Remove leading/trailing whitespaces
    return tweet

# Data augmentation functions
def get_synonyms(word):
    synonyms = set()
    for syn in wordnet.synsets(word):
        for l in syn.lemmas():
            synonym = l.name().replace("_", " ").replace("-", " ").lower()
            synonym = "".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])
            synonyms.add(synonym)
    if word in synonyms:
        synonyms.remove(word)
    return list(synonyms)

def synonym_replacement(words, n):
    new_words = words.copy()
    random_word_list = list(set([word for word in words if word not in stop_words]))
    random.shuffle(random_word_list)
    num_replaced = 0
    for random_word in random_word_list:
        synonyms = get_synonyms(random_word)
        if len(synonyms) >= 1:
            synonym = random.choice(list(synonyms))
            new_words = [synonym if word == random_word else word for word in new_words]
            num_replaced += 1
        if num_replaced >= n:
            break

    sentence = ' '.join(new_words)
    new_words = sentence.split(' ')

    return new_words

def random_deletion(words, p):
    if len(words) == 1:
        return words

    new_words = []
    for word in words:
        r = random.uniform(0, 1)
        if r > p:
            new_words.append(word)

    if len(new_words) == 0:
        rand_int = random.randint(0, len(words)-1)
        return [words[rand_int]]

    return new_words

def random_swap(words, n):
    if len(words) < 2:
        return words
    new_words = words.copy()
    for _ in range(n):
        new_words = swap_word(new_words)
    return new_words

def swap_word(new_words):
    random_idx_1 = random.randint(0, len(new_words)-1)
    random_idx_2 = random_idx_1
    counter = 0
    while random_idx_1 == random_idx_2:
        random_idx_2 = random.randint(0, len(new_words)-1)
        counter += 1
        if counter > 3:
            return new_words
    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]
    return new_words

def random_insertion(words, n):
    new_words = words.copy()
    for _ in range(n):
        add_word(new_words)
    return new_words

def add_word(new_words):
    synonyms = []
    counter = 0
    while len(synonyms) < 1:
        random_word = new_words[random.randint(0, len(new_words)-1)]
        synonyms = get_synonyms(random_word)
        counter += 1
        if counter >= 10:
            return
    random_synonym = random.choice(list(synonyms))
    random_idx = random.randint(0, len(new_words)-1)
    new_words.insert(random_idx, random_synonym)

def augment_sentence(sentence):
    words = sentence.split()
    num_words = len(words)
    if num_words == 0:
        return [sentence]

    augmented_sentences = []
    augmented_sentences.append(' '.join(synonym_replacement(words, max(1, int(0.1*num_words)))))
    augmented_sentences.append(' '.join(random_deletion(words, 0.1)))
    augmented_sentences.append(' '.join(random_swap(words, max(1, int(0.1*num_words)))))
    augmented_sentences.append(' '.join(random_insertion(words, max(1, int(0.1*num_words)))))

    return augmented_sentences

# Load the dataset
file_path = '/content/labeled_data.csv'
data = pd.read_csv(file_path)

# Remove unnecessary columns and clean the data
data = data[['tweet', 'class']]
data['tweet'] = data['tweet'].apply(clean_tweet)

# Encode the labels
label_encoder = LabelEncoder()
data['class'] = label_encoder.fit_transform(data['class'])

# Data augmentation
augmented_data = []

for index, row in data.iterrows():
    tweet = row['tweet']
    label = row['class']
    augmented_sentences = augment_sentence(tweet)
    for augmented_sentence in augmented_sentences:
        augmented_data.append((augmented_sentence, label))

augmented_df = pd.DataFrame(augmented_data, columns=['tweet', 'class'])
data = pd.concat([data, augmented_df])

# Split the data
X_train, X_test, y_train, y_test = train_test_split(data['tweet'], data['class'], test_size=0.2, random_state=42)

# Vectorize the tweet text using TF-IDF
vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Custom training loop to capture neutrosophic scores
def train_mlp_neutrosophic(X_train, y_train, X_val, y_val, max_iter=50, hidden_layer_sizes=(50,), activation='relu', solver='adam', random_state=42):
    mlp_true = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, max_iter=1, activation=activation, solver=solver, warm_start=True, random_state=random_state)
    mlp_false = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, max_iter=1, activation=activation, solver=solver, warm_start=True, random_state=random_state)

    train_neutrosophic_scores = []
    val_neutrosophic_scores = []

    for i in range(max_iter):
        mlp_true.fit(X_train, y_train)
        mlp_false.fit(X_train, 1 - y_train)  # Falsity scores are 1 - Truth scores

        T_train = mlp_true.predict_proba(X_train)
        F_train = mlp_false.predict_proba(X_train)

        # Calculate Indeterminacy (I)
        I_train = 1 - T_train - F_train

        train_neutrosophic_scores.append((T_train, I_train, F_train))

        # Validation scores
        T_val = mlp_true.predict_proba(X_val)
        F_val = mlp_false.predict_proba(X_val)
        I_val = 1 - T_val - F_val

        val_neutrosophic_scores.append((T_val, I_val, F_val))

    return mlp_true, mlp_false, train_neutrosophic_scores, val_neutrosophic_scores

# Train the models with custom neutrosophic training loop
mlp_true, mlp_false, train_neutrosophic_scores, val_neutrosophic_scores = train_mlp_neutrosophic(X_train_tfidf, y_train, X_val_split, y_val_split)

# Extracting loss from neutrosophic scores
train_loss_neutro_true = [log_loss(y_train, T) for T, _, _ in train_neutrosophic_scores]
train_loss_neutro_false = [log_loss(1 - y_train, F) for _, _, F in train_neutrosophic_scores]
val_loss_neutro_true = [log_loss(y_val_split, T) for T, _, _ in val_neutrosophic_scores]
val_loss_neutro_false = [log_loss(1 - y_val_split, F) for _, _, F in val_neutrosophic_scores]

# Function to plot training and validation neutrosophic loss curves
def plot_neutrosophic_curves(train_loss_true, val_loss_true, train_loss_false, val_loss_false):
    plt.figure(figsize=(12, 6))

    # Plot training and validation loss for True Membership Function
    plt.plot(train_loss_true, label='Neutrosophic True Membership Function Training Loss')
    plt.plot(val_loss_true, label='Neutrosophic True Membership Function Validation Loss')

    # Plot training and validation loss for False Membership Function
    plt.plot(train_loss_false, label='Neutrosophic False Membership Function Training Loss')
    plt.plot(val_loss_false, label='Neutrosophic False Membership Function Validation Loss')

    # Adding labels and legend
    plt.title('Neutrosophic Training and Validation Loss Curves')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Plot the training and validation neutrosophic loss curves
plot_neutrosophic_curves(train_loss_neutro_true, val_loss_neutro_true, train_loss_neutro_false, val_loss_neutro_false)

# Function to predict a batch of tweets
def predict_tweets(tweets):
    cleaned_tweets = [clean_tweet(tweet) for tweet in tweets]
    tweets_tfidf = vectorizer.transform(cleaned_tweets)
    T = mlp_true.predict_proba(tweets_tfidf)
    F = mlp_false.predict_proba(tweets_tfidf)
    I = 1 - T - F
    predictions = np.argmax(T, axis=1)
    return predictions

# Predicting a list of tweets using the predict_tweets function
tweets = ["all jews are the same!", "what a dirtsy man!", "Have a nice day!", "What an ugly girl!", "stop fuckking eating"]
predicted_classes = predict_tweets(tweets)

# Convert predicted class indices back to their original labels
predicted_classes_labels = label_encoder.inverse_transform(predicted_classes)

for tweet, predicted_class in zip(tweets, predicted_classes_labels):
    print(f"Tweet: {tweet} -> Predicted Class: {predicted_class}")

import numpy as np
import pandas as pd
import re
import random
import nltk
from nltk.corpus import wordnet, stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import log_loss
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
from pyswarm import pso
from scipy.optimize import differential_evolution

# Ensure NLTK WordNet and stopwords are downloaded
nltk.download('wordnet')
nltk.download('stopwords')

# Define stop words
stop_words = set(stopwords.words('english'))

# Define the clean_tweet function
def clean_tweet(tweet):
    tweet = re.sub(r'http\S+', '', tweet)  # Remove URLs
    tweet = re.sub(r'@\w+', '', tweet)     # Remove mentions
    tweet = re.sub(r'#\w+', '', tweet)     # Remove hashtags
    tweet = re.sub(r'\d+', '', tweet)      # Remove numbers
    tweet = re.sub(r'[^\w\s]', '', tweet)  # Remove punctuation
    tweet = tweet.lower()                  # Convert to lowercase
    tweet = tweet.strip()                  # Remove leading/trailing whitespaces
    return tweet

# Data augmentation functions
def get_synonyms(word):
    synonyms = set()
    for syn in wordnet.synsets(word):
        for l in syn.lemmas():
            synonym = l.name().replace("_", " ").replace("-", " ").lower()
            synonym = "".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])
            synonyms.add(synonym)
    if word in synonyms:
        synonyms.remove(word)
    return list(synonyms)

def synonym_replacement(words, n):
    new_words = words.copy()
    random_word_list = list(set([word for word in words if word not in stop_words]))
    random.shuffle(random_word_list)
    num_replaced = 0
    for random_word in random_word_list:
        synonyms = get_synonyms(random_word)
        if len(synonyms) >= 1:
            synonym = random.choice(list(synonyms))
            new_words = [synonym if word == random_word else word for word in new_words]
            num_replaced += 1
        if num_replaced >= n:
            break

    sentence = ' '.join(new_words)
    new_words = sentence.split(' ')

    return new_words

def random_deletion(words, p):
    if len(words) == 1:
        return words

    new_words = []
    for word in words:
        r = random.uniform(0, 1)
        if r > p:
            new_words.append(word)

    if len(new_words) == 0:
        rand_int = random.randint(0, len(words)-1)
        return [words[rand_int]]

    return new_words

def random_swap(words, n):
    if len(words) < 2:
        return words
    new_words = words.copy()
    for _ in range(n):
        new_words = swap_word(new_words)
    return new_words

def swap_word(new_words):
    random_idx_1 = random.randint(0, len(new_words)-1)
    random_idx_2 = random_idx_1
    counter = 0
    while random_idx_1 == random_idx_2:
        random_idx_2 = random.randint(0, len(new_words)-1)
        counter += 1
        if counter > 3:
            return new_words
    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]
    return new_words

def random_insertion(words, n):
    new_words = words.copy()
    for _ in range(n):
        add_word(new_words)
    return new_words

def add_word(new_words):
    synonyms = []
    counter = 0
    while len(synonyms) < 1:
        random_word = new_words[random.randint(0, len(new_words)-1)]
        synonyms = get_synonyms(random_word)
        counter += 1
        if counter >= 10:
            return
    random_synonym = random.choice(list(synonyms))
    random_idx = random.randint(0, len(new_words)-1)
    new_words.insert(random_idx, random_synonym)

def augment_sentence(sentence):
    words = sentence.split()
    num_words = len(words)
    if num_words == 0:
        return [sentence]

    augmented_sentences = []
    augmented_sentences.append(' '.join(synonym_replacement(words, max(1, int(0.1*num_words)))))
    augmented_sentences.append(' '.join(random_deletion(words, 0.1)))
    augmented_sentences.append(' '.join(random_swap(words, max(1, int(0.1*num_words)))))
    augmented_sentences.append(' '.join(random_insertion(words, max(1, int(0.1*num_words)))))

    return augmented_sentences

# Load the dataset
file_path = '/content/labeled_data.csv'
data = pd.read_csv(file_path)

# Remove unnecessary columns and clean the data
data = data[['tweet', 'class']]
data['tweet'] = data['tweet'].apply(clean_tweet)

# Encode the labels
label_encoder = LabelEncoder()
data['class'] = label_encoder.fit_transform(data['class'])

# Data augmentation
augmented_data = []

for index, row in data.iterrows():
    tweet = row['tweet']
    label = row['class']
    augmented_sentences = augment_sentence(tweet)
    for augmented_sentence in augmented_sentences:
        augmented_data.append((augmented_sentence, label))

augmented_df = pd.DataFrame(augmented_data, columns=['tweet', 'class'])
data = pd.concat([data, augmented_df])

# Split the data
X_train, X_test, y_train, y_test = train_test_split(data['tweet'], data['class'], test_size=0.2, random_state=42)

# Vectorize the tweet text using TF-IDF
vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Whale Optimization Algorithm (WOA) for optimizing MLP
# Optimize MLP using WOA
def optimize_mlp_woa(X_train, y_train, random_state=42):
    # Define the MLP
    mlp = MLPClassifier(max_iter=50, random_state=random_state)

    # Define the objective function
    def objective(params):
        mlp.set_params(hidden_layer_sizes=(int(params[0]),))
        mlp.fit(X_train, y_train)
        return log_loss(y_train, mlp.predict_proba(X_train))

    # Define the parameter bounds
    bounds = [(50, 500)]  # Hidden layer sizes

    # Run WOA optimization
    best_params, _ = pso(objective, *zip(*bounds))

    # Set the best parameters
    mlp.set_params(hidden_layer_sizes=(int(best_params[0]),))

    # Train the MLP with the best parameters
    mlp.fit(X_train, y_train)

    return mlp

# Optimize MLP using WOA
mlp_optimized_woa = optimize_mlp_woa(X_train_tfidf, y_train)


# Particle Swarm Optimization (PSO) for fine-tuning MLP
def fine_tune_mlp_pso(X_train, y_train, mlp, random_state=42):
    # Define the objective function
    def objective(params):
        mlp.set_params(max_iter=int(params[0]))
        mlp.fit(X_train, y_train)
        return log_loss(y_train, mlp.predict_proba(X_train))

    # Define the parameter bounds
    bounds = [(50, 500)]  # Max iterations

    # Run PSO optimization
    best_params, _ = pso(objective, *zip(*bounds))

    # Set the best parameters
    mlp.set_params(max_iter=int(best_params[0]))

    # Train the MLP with the best parameters
    mlp.fit(X_train, y_train)

    return mlp

# Fine-tune the optimized MLP using PSO
mlp_fine_tuned_pso = fine_tune_mlp_pso(X_train_tfidf, y_train, mlp_optimized_woa)

# Function to predict a batch of tweets
def predict_tweets(tweets, model):
    cleaned_tweets = [clean_tweet(tweet) for tweet in tweets]
    tweets_tfidf = vectorizer.transform(cleaned_tweets)
    predictions = model.predict(tweets_tfidf)
    return predictions

# Predicting a list of tweets using the optimized and fine-tuned MLP
tweets = ["all jews are the same!", "what a dirtsy man!", "Have a nice day!", "What an ugly girl!", "stop fuckking eating"]
predicted_classes = predict_tweets(tweets, mlp_fine_tuned_pso)

# Convert predicted class indices back to their original labels
predicted_classes_labels = label_encoder.inverse_transform(predicted_classes)

for tweet, predicted_class in zip(tweets, predicted_classes_labels):
    print(f"Tweet: {tweet} -> Predicted Class: {predicted_class}")